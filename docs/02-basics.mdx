import { Card, Col, Row } from 'antd';

# Basics

## What is front end / web development?

A React developer designs and creates JavaScript-based applications for web or mobile environments

## Set Up 

- Get gmail account if you dont have one yet
- Install latest LTS ( Long Term Stable )- version of Node
- Verify Installation of node and npm
```py
node -v and npm -v
```
- Install VS Code and Extensions if not done yet. Please install the following extensions and then restart VSCode
 Live server, Tebnine, Auto Rename Tag,Bracket Pair Colorizer .Make sure AutoSave is on VSCode

- Create a folder called WriteYourSoftwareHere . This is where we will coding and putting all the projects
- Download and install the Zoom desktop version. Do not use the browser version.
- If you don't have a codepen account , open account

## Terminal Shortcuts - MAC
- Make directory - mkdir
- Where you are right -pwd
- files in this path --ls
- creating a file --touch filename
- able to go back and forth in folder cd ..
- able to go back to root --cd
- deleting a file --rm
- deleting folder-- rmdir
- Windows User
- Creating a file - new-item
- Where you are right -dir
- files in this path -
- deleting - remove-item -recurse ( same for removing folder and file )t

## SHORTCUT KEYS of VSCode
VSCode Shortcut is most underrated but most valuable skill to master.
Exact Feedback from the manager for a candidate received few days back .
```
We will pass on XXXX
We liked XXXX for his tenacity. Also, in face of a hard challenge he did not complain. These are good attributes and will lead him to a good job soon.Unfortunately, he was not fast enough and we are pushing for a high volume of code production. I am sure he will speed up in the future (perhaps even near future) - he just needs to work at it, but we need to know that our candidates are fast workers now. 

XXXX should also spend more time studying and using the latest frameworks. We still like him and wish him luck on his future endeavors.
Thank you, 
XXXX
```
Shortcut keys Show the person you are sharing that you are experienced.
Psychologically it's intimidating to the next person.
These will also speed up and help us code
### Common shortcut keys
- Moving line up and down 
- Copy line above and below 
- Deleting Lining 
- Commenting and uncommenting

### LESS COMMON
- Multi-Select and multi-line edit 
- Control Shift F to search the entire code --- This will be your most used command as an engineer.In most instances whatever story you are assigned has been done before, So all you have to do is a copy. For Copying, you need to search the entire code for keywords.
- Going to top and bottom of the Page - Real life components are so large that it will take forever to go up and down – PRODUCTION
- Fold and Unfold functions - PRODUCTION
For Ex: If your story is to change the font size of the header, You will search the entire code looking for the header to see all instances of it, make sure and apply changes.

```
Homework: Practice shortcut keys for at least 1 Hour. I don’t want to see anyone not using shortcut keys from tomorrow
Take VSCode and terminal short cut keys
```


![](/img/concepts/dataengarch.gif)

The first type of data engineering is SQL-focused. The work and primary storage of the data is in relational databases. All of the data processing is done with SQL or a SQL-based language. Sometimes, this data processing is done with an ETL tool. The second type of data engineering is Big Data–focused. The work and primary storage of the data is in Big Data technologies like Hadoop, Cassandra, and HBase. All of the data processing is done in Big Data frameworks like MapReduce, Spark, and Flink. While SQL is used, the primary processing is done with programming languages like Java, Scala, and Python.

<iframe width="100%" height="480" src="https://www.youtube.com/embed/qWru-b6m030" title="How Data Engineering Works" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

## Why we need Data Engineers?

Internet companies create a lot of data. Datasets are getting larger and messier in he internet era where there are large datasets for all the actions and interactions users make with your website all the way to product descriptions, images, time series info, comments etc.

It is said that data is the new oil. For natural oil, we also need a way to drill and mine this oil for it to be useful. In the same way, we need a way to mine and make sense of all this data to be useful.

On one had, there is a desire by executives and management to get insights from these datasets.

There is also a desire by data scientists and ML practitioners to have clean datasets to model with.

There are some really interesting trade-offs to make when you do this. And knowing about these can help you in your own journey as a data scientist or ML person working with large data. Irrespective of where you are in your data journey, I think you will find these interesting.

## Roles in Data Teams

> Data engineers are in the business of moving data—either getting it from one location to another or transforming the data in some man‐ ner. It is these hard workers who provide the digital grease that makes a data project a reality.

<iframe width="100%" height="480" src="https://www.youtube.com/embed/m5hLUknIi5c" title="Roles in Data Science Teams" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

## How a typical Data Engineering Pipeline looks

This is one example of a standard data pipeline in AWS ecosystem:

![](/img/frameworks/data_engg_aws.drawio.svg)
![](/img/frameworks/data_engg_aws_notes.drawio.svg)

## Know more about Data Engineering Basics

1. [Data Engineering Roadmap](https://knowledgetree.notion.site/Data-Engineering-Roadmap-6e543497f9074aba89520b45b678d32f)
2. [Approaching the data pipeline architecture](https://knowledgetree.notion.site/Approaching-the-data-pipeline-architecture-214bdf596037454ca3f879894035c83f)
3. [The Data Engineering Megatrend: A Brief History](https://www.rudderstack.com/blog/the-data-engineering-megatrend-a-brief-history)
4. [How to gather requirements for your data project](https://www.startdataengineering.com/post/n-questions-data-pipeline-req/)
5.  [Five Steps to land a high paying data engineering job](https://www.startdataengineering.com/post/n-steps-high-pay-de-job/)

---

## Our Learning Framework

![](/img/learning_framework_embedded.svg)


### Learning

Learning will happen under the guided instructor-led sessions. We will do the following activites to learn the core concepts of data engineering:

1. Theory and Case: We will find the answers to - 1) What is the particular technology/tool/concept is about, and 2) Why we are learnig about that? How things works in real-life is important and student should understand this very well. Along with the theory, we will also understand how companies/people uses the given technology/tool/concept in real-world to solve their business problems.
2. Lab and Presentation: A guided session with instructur where instructor will show the process step-by-step in real-time and students will ask questions and clarify their doubts. Each lab will be broken down into steps after each step, each student will present that step in their own system/environment with some variations (if possible).
   
### Assessment

Student assessment is the most important factor in tracking the progress of the students. We will do the assessment using the following methods and drills:

1. Mock Interview (MI): This is a 1:1 mock interview between the instructor and the student. Student will be assessed based on the quality of responses.
2. Peer Interview (PI): This is a 1:1 peer interview between a pair of students. Student will be assessed based on the quality of questions and quality of responses.
3. Lab Assignment (LA): A hypothetical data engineering business case will be provided and the student has to apply the data engineering skills to solve the business requirement. Student will be assessed based on the assignment-specific factors and timely submission.
4. Capstone Project (CP): Student will pick a brand/company persona and create a hypothetical scenario. Then the student will solve the scenario and submit the code for review. Student will be assessed based on the topic-specific factors and timely submission.

## Every Week
- New Labs
- Full Mock Interviews
- Resume Buildups
- Essential Skill sessions
- Take-home Assignments
- Capstone Project Discussion
- Industrial Case Study Discussion
- General Discussion

## Skills

### SQL

Querying data using SQL is an essential skill for anyone who works with data.

Listed as one of the top technologies on data engineer job listings, SQL is a standardized programming language used to manage relational databases (not exclusively) and perform various operations on the data in them. Data Engineering using Spark SQL.

### Programming Language

As a data engineer you'll be writing a lot of code to handle various business cases such as ETLs, data pipelines, etc. The de facto standard language for data engineering is Python (not to be confused with R or nim that are used for data science, they have no use in data engineering).

Python helps data engineers to build efficient data pipelines as many data engineering tools use Python in the backend. Moreover, various tools in the market are compatible with Python and allow data engineers to integrate them into their everyday tasks by simply learning Python.

### Cloud Computing

Data engineers are expected to be able to process and handle data efficiently. Many companies prefer the cloud solutions to the on-premise ones. Amazon Web Services (AWS) is the world's most comprehensive and broadly adopted cloud platform, offering over 200 fully featured services.

Data engineers need to meet various requirements to build data pipelines. This is where AWS data engineering tools come into the scenario. AWS data engineering tools make it easier for data engineers to build AWS data pipelines, manage data transfer, and ensure efficient data storage.

### Shell Scripting

Unix machines are used everywhere these days and as a data engineer, they are something we interact with every single day. Understanding common shell tools and how to use them to manipulate data will increase your flexibility and speed when dealing with day to day data activities.

### Git

Git is one of the skills that every software engineer needs to manage the code base efficiently. GitHub or any version control software is important for any software development projects, including those which are data driven. GitHub allows version control of your projects through Git.

### VSCode

The VSCode provides rich functionalities, extensions (plugins), built-in Git, ability to run and debug code, and complete customization for the workspace. You can build, test, deploy, and monitor your data engineering applications and pipelines without leaving the application.

### Jupyter Notebook

Jupyter Notebook is one of the most widely used tool in data engineering that use Python. This is due to the fact, that it is an ideal environment for developing reproducible data pipelines with Python. Colaboratory, or “Colab” for short, is a provide Jupyter-like environment on the cloud.

### Relational Databases - Design & Architecture

RDBMS are the basic building blocks for any application data. A data engineer should know how to design and architect their structures, and learn about concepts that are related to them.

### NoSQL Databases - Design & Architecture

NoSQL is a term for any non-relational database model: key-value, document, column, graph, and more. A basic acquaintance is required, but going deeper into any model depends on the job.

### Columnar Databases

Column databases are a kind of nosql databases. They deserve their own section as they are essential for the data engineer as working with Big Data online (as opposed to offline batching) usually requires a columnar back-end.

### Data Warehouses

Understand the concepts behind data warehouses and familiarize youself with common data warehouse solutions.

### OLAP Data Modeling

OLAP (analytical) databases (used in data warehouses) data modeling concepts, modeling the data correctly is essential for a functioning data warehouse.

### Data Lakes and Lakehouses

### Batch Data Processing

Data processing using Python, SQL and Spark. Everyone should know how it works, but going deep into the details and operations are recommended only if necessary.

### Stream Data Processing

Data processing on the fly. Suggested to get a good grasp of the subject and then dive deep into a specific tool like Kafka, Spark, Flink, etc.

### Pipeline / Workflow Management

Scheduling tools for data processing. Airflow is considered to be the defacto standard, but any understanding of DAGs - directed acyclical graphs for tasks will be good.

### Infra as Code

### Container Orchestration

### API Management

### CICD Pipelines

### MLOps Pipelines

### Machine Learning Basics

### Deep Learning Basics - NLP and Computer Vision